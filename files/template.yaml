{{#with cluster}}
ESCluster:
  Type: AWS::Elasticsearch::Domain
  Properties:
    DomainName: {{domain}}
    ElasticsearchVersion: "6.3"
    EBSOptions:
      EBSEnabled: true
      VolumeType: gp2
      {{#if diskSize}}
      VolumeSize: {{diskSize}}
      {{/if}}
    {{#if cluster}}
    ElasticsearchClusterConfig:
      {{#with master}}
      DedicatedMasterCount: {{count}}
      DedicatedMasterEnabled: true
      DedicatedMasterType: {{type}}
      {{/with}}
      {{#with nodes}}
      InstanceCount: {{count}}
      InstanceType: {{type}}
      ZoneAwarenessEnabled: {{highAvailability}}
      {{/with}}
    {{/if}}
    {{#if snapshot}}
    SnapshotOptions:
      AutomatedSnapshotStartHour: 0
    {{/if}}
    Tags:
      {{#each @root.tags}}
      - Key: {{Key}}
        Value: {{Value}}
      {{/each}}
{{/with}}


{{#with stream}}
{{#if newBucket}}
FirehoseBackupBucket:
  Type: "AWS::S3::Bucket"
  DeletionPolicy: Retain
  Properties:
    BucketName: {{bucket}}
{{/if}}

# TODO: This was mostly taken from the CloudWatch-to-Firehose Lambda blueprint, which is exhaustive and complete, but also limited to Node 6 and makes tedious use of thenables.  Consider updating this or keeping an eye out for a new version
FirehoseLambda:
  Type: "AWS::Lambda::Function"
  Properties:
    Handler: "index.handler"
    Role: !GetAtt GetApiKeyValueFunctionRole.Arn
    Runtime: nodejs6.10
    Timeout: 120
    # Environment:
    #   Variables:
    #     API_KEY: !Ref ApiKey
    Code:
      ZipFile: !Sub |
        /*
        For processing data sent to Firehose by Cloudwatch Logs subscription filters.

        Cloudwatch Logs sends to Firehose records that look like this:

        {
          "messageType": "DATA_MESSAGE",
          "owner": "123456789012",
          "logGroup": "log_group_name",
          "logStream": "log_stream_name",
          "subscriptionFilters": [
            "subscription_filter_name"
          ],
          "logEvents": [
            {
              "id": "01234567890123456789012345678901234567890123456789012345",
              "timestamp": 1510109208016,
              "message": "log message 1"
            },
            {
              "id": "01234567890123456789012345678901234567890123456789012345",
              "timestamp": 1510109208017,
              "message": "log message 2"
            }
            ...
          ]
        }

        The data is additionally compressed with GZIP.

        The code below will:

        1) Gunzip the data
        2) Parse the json
        3) Set the result to ProcessingFailed for any record whose messageType is not DATA_MESSAGE, thus redirecting them to the
           processing error output. Such records do not contain any log events. You can modify the code to set the result to
           Dropped instead to get rid of these records completely.
        4) For records whose messageType is DATA_MESSAGE, extract the individual log events from the logEvents field, and pass
           each one to the transformLogEvent method. You can modify the transformLogEvent method to perform custom
           transformations on the log events.
        5) Concatenate the result from (4) together and set the result as the data of the record returned to Firehose. Note that
           this step will not add any delimiters. Delimiters should be appended by the logic within the transformLogEvent
           method.
        6) Any additional records which exceed 6MB will be re-ingested back into Firehose.
        */

        'use strict';

        const zlib = require('zlib');
        const AWS = require('aws-sdk');

        /**
         * logEvent has this format:
         *
         * {
         *   "id": "01234567890123456789012345678901234567890123456789012345",
         *   "timestamp": 1510109208016,
         *   "message": "log message 1"
         * }
         *
         * The default implementation below just extracts the message and appends a newline to it.
         *
         * The result must be returned in a Promise.
         */

         { id: '34302929592210038195232915113689409337104556449623506947',
               timestamp: 1538196561901,
               message: 'REPORT RequestId: 080f5c20-c3a3-11e8-b2c5-4bb528fabb78\tDuration: 113.70 ms\tBilled Duration: 200 ms \tMemory Size: 128 MB\tMax Memory Used: 112 MB\t\n' }

         { id: '34302929590871993483321077725197266240745654759264681985',
                timestamp: 1538196561841,
                message: '2018-09-29T04:49:21.800Z\t080f5c20-c3a3-11e8-b2c5-4bb528fabb78\tINFO Detected a Cuddle Monkey preheater invocation. Short circuting request cycle.\n' }

        function parseRecord(handler, record){
          let message = record.message;
          if (RegExp("^REPORT").test message) {
            return {
              Handler: handler,
              RequestId: RegExp("^REPORT RequestId\: (.*?)\t").exec(message)[1],
              Duration: RegExp("\tDuration: (.*?) ms\t").exec(message)[1],
              Memory: RegExp("\tMax Memory Used: (.*?) MB\t").exec(message)[1],
              Message: message
            }
          } else {
            return {
              Handler: handler
              RequestId: RegExp("^(.*?)\t(.*?)\t").exec(message)[2],
              Message: message
            }
          }
        }

        function transformLogEvent(logGroup){
          let handler = logGroup.slice(12);
          return function (logEvent) {
            out = parseRecord(handler, logEvent);
            return Promise.resolve(JSON.stringify(out) + "\n");
          };
        }



        function putRecordsToFirehoseStream(streamName, records, client, resolve, reject, attemptsMade, maxAttempts) {
            client.putRecordBatch({
                DeliveryStreamName: streamName,
                Records: records,
            }, (err, data) => {
                const codes = [];
                let failed = [];
                let errMsg = err;

                if (err) {
                    failed = records;
                } else {
                    for (let i = 0; i < data.RequestResponses.length; i++) {
                        const code = data.RequestResponses[i].ErrorCode;
                        if (code) {
                            codes.push(code);
                            failed.push(records[i]);
                        }
                    }
                    errMsg = `Individual error codes: ${codes}`;
                }

                if (failed.length > 0) {
                    if (attemptsMade + 1 < maxAttempts) {
                        console.log('Some records failed while calling PutRecordBatch, retrying. %s', errMsg);
                        putRecordsToFirehoseStream(streamName, failed, client, resolve, reject, attemptsMade + 1, maxAttempts);
                    } else {
                        reject(`Could not put records after ${maxAttempts} attempts. ${errMsg}`);
                    }
                } else {
                    resolve('');
                }
            });
        }

        function putRecordsToKinesisStream(streamName, records, client, resolve, reject, attemptsMade, maxAttempts) {
            client.putRecords({
                StreamName: streamName,
                Records: records,
            }, (err, data) => {
                const codes = [];
                let failed = [];
                let errMsg = err;

                if (err) {
                    failed = records;
                } else {
                    for (let i = 0; i < data.Records.length; i++) {
                        const code = data.Records[i].ErrorCode;
                        if (code) {
                            codes.push(code);
                            failed.push(records[i]);
                        }
                    }
                    errMsg = `Individual error codes: ${codes}`;
                }

                if (failed.length > 0) {
                    if (attemptsMade + 1 < maxAttempts) {
                        console.log('Some records failed while calling PutRecords, retrying. %s', errMsg);
                        putRecordsToKinesisStream(streamName, failed, client, resolve, reject, attemptsMade + 1, maxAttempts);
                    } else {
                        reject(`Could not put records after ${maxAttempts} attempts. ${errMsg}`);
                    }
                } else {
                    resolve('');
                }
            });
        }

        function createReingestionRecord(isSas, originalRecord) {
            if (isSas) {
                return {
                    Data: new Buffer(originalRecord.data, 'base64'),
                    PartitionKey: originalRecord.kinesisRecordMetadata.partitionKey,
                };
            } else {
                return {
                    Data: new Buffer(originalRecord.data, 'base64'),
                };
            }
        }


        function getReingestionRecord(isSas, reIngestionRecord) {
            if (isSas) {
                return {
                    Data: reIngestionRecord.Data,
                    PartitionKey: reIngestionRecord.PartitionKey,
                };
            } else {
                return {
                    Data: reIngestionRecord.Data,
                };
            }
        }

        exports.handler = (event, context, callback) => {
            Promise.all(event.records.map(r => {
                const buffer = new Buffer(r.data, 'base64');
                const decompressed = zlib.gunzipSync(buffer);
                const data = JSON.parse(decompressed);
                // CONTROL_MESSAGE are sent by CWL to check if the subscription is reachable.
                // They do not contain actual data.
                if (data.messageType === 'CONTROL_MESSAGE') {
                    return Promise.resolve({
                        recordId: r.recordId,
                        result: 'Dropped',
                    });
                } else if (data.messageType === 'DATA_MESSAGE') {
                    const promises = data.logEvents.map(transformLogEvent(data.logGroup));
                    return Promise.all(promises)
                        .then(transformed => {
                            const payload = transformed.reduce((a, v) => a + v, '');
                            const encoded = new Buffer(payload).toString('base64');
                            return {
                                recordId: r.recordId,
                                result: 'Ok',
                                data: encoded,
                            };
                        });
                } else {
                    return Promise.resolve({
                        recordId: r.recordId,
                        result: 'ProcessingFailed',
                    });
                }
            })).then(recs => {
                const isSas = Object.prototype.hasOwnProperty.call(event, 'sourceKinesisStreamArn');
                const streamARN = isSas ? event.sourceKinesisStreamArn : event.deliveryStreamArn;
                const region = streamARN.split(':')[3];
                const streamName = streamARN.split('/')[1];
                const result = { records: recs };
                let recordsToReingest = [];
                const putRecordBatches = [];
                let totalRecordsToBeReingested = 0;
                const inputDataByRecId = {};
                event.records.forEach(r => inputDataByRecId[r.recordId] = createReingestionRecord(isSas, r));

                let projectedSize = recs.filter(rec => rec.result === 'Ok')
                                      .map(r => r.recordId.length + r.data.length)
                                      .reduce((a, b) => a + b);
                // 6000000 instead of 6291456 to leave ample headroom for the stuff we didn't account for
                for (let idx = 0; idx < event.records.length && projectedSize > 6000000; idx++) {
                    const rec = result.records[idx];
                    if (rec.result === 'Ok') {
                        totalRecordsToBeReingested++;
                        recordsToReingest.push(getReingestionRecord(isSas, inputDataByRecId[rec.recordId]));
                        projectedSize -= rec.data.length;
                        delete rec.data;
                        result.records[idx].result = 'Dropped';

                        // split out the record batches into multiple groups, 500 records at max per group
                        if (recordsToReingest.length === 500) {
                            putRecordBatches.push(recordsToReingest);
                            recordsToReingest = [];
                        }
                    }
                }

                if (recordsToReingest.length > 0) {
                    // add the last batch
                    putRecordBatches.push(recordsToReingest);
                }

                if (putRecordBatches.length > 0) {
                    new Promise((resolve, reject) => {
                        let recordsReingestedSoFar = 0;
                        for (let idx = 0; idx < putRecordBatches.length; idx++) {
                            const recordBatch = putRecordBatches[idx];
                            if (isSas) {
                                const client = new AWS.Kinesis({ region: region });
                                putRecordsToKinesisStream(streamName, recordBatch, client, resolve, reject, 0, 20);
                            } else {
                                const client = new AWS.Firehose({ region: region });
                                putRecordsToFirehoseStream(streamName, recordBatch, client, resolve, reject, 0, 20);
                            }
                            recordsReingestedSoFar += recordBatch.length;
                            console.log('Reingested %s/%s records out of %s in to %s stream', recordsReingestedSoFar, totalRecordsToBeReingested, event.records.length, streamName);
                        }
                    }).then(
                      () => {
                          console.log('Reingested all %s records out of %s in to %s stream', totalRecordsToBeReingested, event.records.length, streamName);
                          callback(null, result);
                      },
                      failed => {
                          console.log('Failed to reingest records. %s', failed);
                          callback(failed, null);
                      });
                } else {
                    console.log('No records needed to be reingested.');
                    callback(null, result);
                }
            }).catch(ex => {
                console.log('Error: ', ex);
                callback(ex, null);
            });
        };


FirehoseRole:
  DependsOn:
    - ESCluster
  Type: "AWS::IAM::Role"
  Properties:
    AssumeRolePolicyDocument:
      Version: "2012-10-17"
      Statement:
        - Effect: "Allow"
          Principal:
            Service:
              - firehose.amazonaws.com
          Action:
            - "sts:AssumeRole"
    Policies:
      - PolicyName: {{name}}-role
        PolicyDocument:
          Version: "2012-10-17"
          Statement:
            - Effect: Allow
              Action: [
                "s3:AbortMultipartUpload",
                "s3:GetBucketLocation",
                "s3:GetObject",
                "s3:ListBucket",
                "s3:ListBucketMultipartUploads",
                "s3:PutObject"
              ]
              Resource: [
                "arn:aws:s3:::{{bucket}}",
                "arn:aws:s3:::{{bucket}}/*"
              ]
            - Effect: Allow
              Action: [
                "es:DescribeElasticsearchDomain",
                "es:DescribeElasticsearchDomains",
                "es:DescribeElasticsearchDomainConfig",
                "es:ESHttpPost",
                "es:ESHttpPut"
              ]
              Resource: [
                "arn:aws:es:*:{{@root.accountID}}:domain/{{@root.cluster.domain}}",
                "arn:aws:es:*:{{@root.accountID}}:domain/{{@root.cluster.domain}}/*"
              ]
            - Effect: Allow
              Action: [
                "es:ESHttpGet"
              ]
              Resource: [
                "arn:aws:es:*:{{@root.accountID}}:domain/{{@root.cluster.domain}}/_all/_settings",
                "arn:aws:es:*:{{@root.accountID}}:domain/{{@root.cluster.domain}}/_cluster/stats",
                "arn:aws:es:*:{{@root.accountID}}:domain/{{@root.cluster.domain}}/index-name*/_mapping/type-name",
                "arn:aws:es:*:{{@root.accountID}}:domain/{{@root.cluster.domain}}/_nodes",
                "arn:aws:es:*:{{@root.accountID}}:domain/{{@root.cluster.domain}}/_nodes/stats",
                "arn:aws:es:*:{{@root.accountID}}:domain/{{@root.cluster.domain}}/_nodes/*/stats",
                "arn:aws:es:*:{{@root.accountID}}:domain/{{@root.cluster.domain}}/_stats",
                "arn:aws:es:*:{{@root.accountID}}:domain/{{@root.cluster.domain}}/index-name*/_stats"
              ]


Firehose:
  DependsOn:
    - ESCluster
    - FirehoseRole
  Type: AWS::KinesisFirehose::DeliveryStream
  Properties:
    DeliveryStreamName: {{name}}
    DeliveryStreamType: DirectPut
    ElasticsearchDestinationConfiguration:
      # TODO: These are the defaults from the reference docs.  Is this something that should be part of the mixin configuration interface?
      BufferingHints:
        IntervalInSeconds: 300
        SizeInMBs: 5
      CloudWatchLoggingOptions:
        Enabled: false
      DomainARN:
        "Fn::GetAtt": [ESCluster, DomainArn]
      IndexName: logs
      IndexRotationPeriod: OneWeek
      RetryOptions:
        DurationInSeconds: 7200
      RoleARN:
        "Fn::GetAtt": [FirehoseRole, Arn]
      S3BackupMode: FailedDocumentsOnly
      S3Configuration:
        BucketARN: "arn:aws:s3:::{{bucket}}"
        BufferingHints:
          IntervalInSeconds: 300
          SizeInMBs: 5
        CloudWatchLoggingOptions:
          Enabled: false
        CompressionFormat: ZIP
        EncryptionConfiguration:
          NoEncryptionConfig: NoEncryption
        RoleARN:
          "Fn::GetAtt": [FirehoseRole, Arn]
      TypeName: logs

LogRole:
  DependsOn:
    - Firehose
  Type: "AWS::IAM::Role"
  Properties:
    AssumeRolePolicyDocument:
      Version: "2012-10-17"
      Statement:
        - Effect: "Allow"
          Principal:
            Service:
              - "logs.{{@root.region}}.amazonaws.com"
          Action:
            - "sts:AssumeRole"
    Policies:
      - PolicyName: {{name}}-log-role
        PolicyDocument:
          Version: "2012-10-17"
          Statement:
            - Effect: Allow
              Action: ["firehose:*"]
              Resource: ["arn:aws:firehose:{{@root.region}}:{{@root.accountID}}:*"]
            - Effect: Allow
              Action: ["iam:PassRole"]
              Resource: ["arn:aws:iam::{{@root.accountID}}:role/{{name}}-log-role"]

{{/with}}


{{#each logs}}
SubscriptonFilter{{templateName}}:
  DependsOn:
    - Firehose
  Type: AWS::Logs::SubscriptionFilter
  Properties:
    DestinationArn:
      "Fn::GetAtt": [Firehose, Arn]
    FilterPattern: "?REPORT ?INFO ?WARN ?ERROR"
    LogGroupName: {{name}}
    RoleArn:
      "Fn::GetAtt": [LogRole, Arn]
{{/each}}
